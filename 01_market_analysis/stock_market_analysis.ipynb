{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "FgyU0xSrx_JI",
      "metadata": {
        "id": "FgyU0xSrx_JI"
      },
      "source": [
        "Mount The Google Drive to read the sp500csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ytYSEglxrXRd",
      "metadata": {
        "id": "ytYSEglxrXRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc9ea68-6aa0-474e-8cbf-1318d53dc558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OJlO-RlGyKlG",
      "metadata": {
        "id": "OJlO-RlGyKlG"
      },
      "source": [
        "Command to verfiy if the file exists or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9H3Ck12sfuO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9H3Ck12sfuO",
        "outputId": "f3451137-8005-4bda-c125-cbb6c72ae9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/My Drive/Major Project Sem 8/Mid Sem Eval/sp500.csv': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/My Drive/Major Project Sem 8/Mid Sem Eval/sp500.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ptgyk3Qt4U7T",
      "metadata": {
        "id": "Ptgyk3Qt4U7T"
      },
      "source": [
        "# INTRODUCTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n-fgKnL1ySr5",
      "metadata": {
        "id": "n-fgKnL1ySr5"
      },
      "source": [
        "\n",
        "Predictin the stock market using machine learning.\n",
        "Data is taken from S&P500 index, which is a stock market index tracking the stock performance of 500 of the largest companies listed on stock exchanges in the United States.\n",
        "Overview of the work done in this notebook.\n",
        "1. Clean the data.\n",
        "2. Train the model.\n",
        "3. Backtesting(To find how well our model works)\n",
        "4. Add more predictors to improve our accuracy.\n",
        "5. Pointers to improve the model further.\n",
        "\n",
        "In the end we'll predit the tomorrows price on the basis of previous data.\n",
        "\n",
        "A stock, also known as a share or equity, represents ownership in a specific company. When you buy a stock, you essentially become a shareholder in that company, which means you own a portion of the company's assets.  \n",
        "An index is a statistical measure of the performance of a basket of stocks or securities representing a particular market or sector. It is used to track and benchmark the overall performance of the market or a specific segment of the market.  \n",
        "Investors can buy and sell stocks, but they cannot directly buy or sell an index."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZgagkrGm2_XF",
      "metadata": {
        "id": "ZgagkrGm2_XF"
      },
      "source": [
        "# Downloading The Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53fcbbf-a62d-4aa2-be7b-9aa4e17a5e48",
      "metadata": {
        "id": "f53fcbbf-a62d-4aa2-be7b-9aa4e17a5e48"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf # yahoo finance api to download daily stock prices\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4d2b51-1187-44c7-a281-da84d0381dd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0c4d2b51-1187-44c7-a281-da84d0381dd5",
        "outputId": "3a1e6100-055e-4821-9fca-c3119423d3ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Major Project Sem 8/Mid Sem Eval/sp500.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0954db1747b1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Major Project Sem 8/Mid Sem Eval/sp500.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msp500\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msp500\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTicker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^GSPC\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# GSPC symbol for SandP index in yahoo api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Major Project Sem 8/Mid Sem Eval/sp500.csv'"
          ]
        }
      ],
      "source": [
        "file_path = '/content/drive/My Drive/Major Project Sem 8/Mid Sem Eval/sp500.csv'\n",
        "if len(file_path):\n",
        "    sp500 = pd.read_csv(file_path, index_col=0)\n",
        "else:\n",
        "    sp500 = yf.Ticker(\"^GSPC\") # GSPC symbol for SandP index in yahoo api\n",
        "    sp500 = sp500.history(period=\"max\")#  querying all data from the beginning, gives out a pandas data frame\n",
        "    sp500.to_csv(\"sp500.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f605b43c-8db9-43ae-b2d4-29fc6048b8aa",
      "metadata": {
        "id": "f605b43c-8db9-43ae-b2d4-29fc6048b8aa"
      },
      "outputs": [],
      "source": [
        "sp500.index = pd.to_datetime(sp500.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1X43LypT2Pqq",
      "metadata": {
        "id": "1X43LypT2Pqq"
      },
      "source": [
        "## Explaining the Database\n",
        "\n",
        "Date(index):  \n",
        "Open: opening price of the day  \n",
        "high: highest price of the day  \n",
        "low: lowest   \n",
        "close: closing   \n",
        "vol: amount of shares exchanged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb162ce-7d18-4c14-b349-9014c0d6db42",
      "metadata": {
        "id": "fbb162ce-7d18-4c14-b349-9014c0d6db42"
      },
      "outputs": [],
      "source": [
        "sp500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7ddd5b-9c2d-4c13-8210-72a4adb61159",
      "metadata": {
        "id": "3e7ddd5b-9c2d-4c13-8210-72a4adb61159"
      },
      "outputs": [],
      "source": [
        "sp500.plot.line(y=\"Close\", use_index=True) # show the closing price over the years"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kZS8Y7ZZ3XLP",
      "metadata": {
        "id": "kZS8Y7ZZ3XLP"
      },
      "source": [
        "# Cleaning The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H2CCKMIa3azb",
      "metadata": {
        "id": "H2CCKMIa3azb"
      },
      "source": [
        "These are more appropriate for individual stocks and not an index so these are irrelevant to us. Hence removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b02b64d-9eef-489f-894d-26f9e6792db5",
      "metadata": {
        "id": "1b02b64d-9eef-489f-894d-26f9e6792db5"
      },
      "outputs": [],
      "source": [
        "del sp500[\"Dividends\"]\n",
        "del sp500[\"Stock Splits\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bF5E9kU74yHK",
      "metadata": {
        "id": "bF5E9kU74yHK"
      },
      "source": [
        "Setting up the target for prediction.  \n",
        "For now we are predicting if the price will go up or down tomorrow.  \n",
        "The other method is to predict the absolute price, but the problem with that even if can very accurately predict the absolute price, but lose money since there is no knowledge about if it's +ve or -ve.   \n",
        "\n",
        "So for simplifying the problem and taking the 1st step as to figure out if the price will go up or down.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25e1594-1cd2-47ae-bb50-a0d17ac35c69",
      "metadata": {
        "id": "f25e1594-1cd2-47ae-bb50-a0d17ac35c69"
      },
      "outputs": [],
      "source": [
        "sp500[\"Tomorrows_Close\"] = sp500[\"Close\"].shift(-1) # to show next day's closing price on a particular day"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Oy8CvT46a78",
      "metadata": {
        "id": "3Oy8CvT46a78"
      },
      "source": [
        "## Setting Up The Target\n",
        "Adding a boolean field to know if Tomorrow's closing price was > today's closing price\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871b16a1-9d77-40c8-8564-1947b438a113",
      "metadata": {
        "id": "871b16a1-9d77-40c8-8564-1947b438a113"
      },
      "outputs": [],
      "source": [
        "sp500[\"Target\"] = (sp500[\"Tomorrows_Close\"] > sp500[\"Close\"]).astype(int) # typecasting into int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xgsc2_kN7HOd",
      "metadata": {
        "id": "Xgsc2_kN7HOd"
      },
      "source": [
        "> Note: It is good to have a data that goes far back in history but going too far back has it's downsides because markets changes after a few decades, or there can be incidents like stock market crashes. For now we are keeping these things out of the scope. ANd hence removeing the data that came before 1990."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e58626-3be6-45f7-b1aa-1786593e3bd6",
      "metadata": {
        "id": "79e58626-3be6-45f7-b1aa-1786593e3bd6"
      },
      "outputs": [],
      "source": [
        "sp500 = sp500.loc[\"1990-01-01\":].copy() # take the indexes in range [1990, present]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ogxdhbJs79yy",
      "metadata": {
        "id": "ogxdhbJs79yy"
      },
      "outputs": [],
      "source": [
        "sp500"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BjjuQxkm8DMG",
      "metadata": {
        "id": "BjjuQxkm8DMG"
      },
      "source": [
        "#  Training the Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r8FlGQXH8Hvp",
      "metadata": {
        "id": "r8FlGQXH8Hvp"
      },
      "source": [
        "Random Forest Classifier used because\n",
        "1. it works by training bunch of individual decision trees with randomized parameters. and then averagin the results from those decision trees.\n",
        "1. therefore they are resistant to overfitting, can pick up non linear tendencies in the data as well also faster as it paraleely trains different decision trees.\n",
        "1. non linear relationships\n",
        "\n",
        "Other possible options are SVM, Support vector machine.\n",
        "Gradient boosting trees can be more accurate than random forests. Because we train them to correct each other's errors, they're capable of capturing complex patterns in the data. However, if the data are noisy, the boosted trees may overfit and start modeling the noise\n",
        "SVM is a powerful algorithm for both classification and regression tasks. It works well in high-dimensional spaces and is effective in cases where the decision boundary is not necessarily linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad95d689-520a-4141-ab6e-a0fe3f9697a6",
      "metadata": {
        "id": "ad95d689-520a-4141-ab6e-a0fe3f9697a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# n_estimators=no. of decision trees(try increasing), min_samples = protection against overfitting, if we rerun the model again we get the same results\n",
        "model = RandomForestClassifier(n_estimators=100, min_samples_split=100, random_state=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "znreTB44_r8K",
      "metadata": {
        "id": "znreTB44_r8K"
      },
      "source": [
        "## Splitting the data into Train and test set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jnNdKbTaAAhK",
      "metadata": {
        "id": "jnNdKbTaAAhK"
      },
      "source": [
        "Done to prevent leakage of data into the model. Like predicting the result of the data that we have already used to train that can give good results on the test data but on new data the results will be very bad. The model should not have any information about the future to predict hte future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z1czJEdD_zaK",
      "metadata": {
        "id": "z1czJEdD_zaK"
      },
      "outputs": [],
      "source": [
        "train = sp500.iloc[:-100] # all rows except the last 100 into train set\n",
        "test = sp500.iloc[-100:] # last 100 into test set\n",
        "# Not using the Tomorrows_closing and even the target column in the train set,\n",
        "# since it wont be available in real time prediction\n",
        "predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
        "# use the predictors to predict the Target\n",
        "model.fit(train[predictors], train[\"Target\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ysWH-nuNBgt0",
      "metadata": {
        "id": "ysWH-nuNBgt0"
      },
      "source": [
        "## Checking the Accuracy of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0837787-5d4e-4a20-ad0d-3a546bc23cdb",
      "metadata": {
        "id": "c0837787-5d4e-4a20-ad0d-3a546bc23cdb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "preds = model.predict(test[predictors])\n",
        "preds = pd.Series(preds, index=test.index)\n",
        "# print(preds)\n",
        "precision_score(test[\"Target\"], preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aVRmzilODY-O",
      "metadata": {
        "id": "aVRmzilODY-O"
      },
      "source": [
        "> Not a good precision score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n5uX9LF-Dsys",
      "metadata": {
        "id": "n5uX9LF-Dsys"
      },
      "source": [
        "### Visulaising the predictions\n",
        "compairing the Target in the test set and the preds made from the model on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33e349f-8365-4282-91db-3b5824e83262",
      "metadata": {
        "id": "e33e349f-8365-4282-91db-3b5824e83262"
      },
      "outputs": [],
      "source": [
        "combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
        "combined.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Ln3y7oRfLDs",
      "metadata": {
        "id": "0Ln3y7oRfLDs"
      },
      "outputs": [],
      "source": [
        "print(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9_7fmcspEGZP",
      "metadata": {
        "id": "9_7fmcspEGZP"
      },
      "source": [
        "# Backtesting the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9jqrSeMhEYFt",
      "metadata": {
        "id": "9jqrSeMhEYFt"
      },
      "source": [
        "So that model cover up some cases and be can be more confident in predicting the prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4049497-0ee7-4399-83ab-ef61ccf71133",
      "metadata": {
        "id": "d4049497-0ee7-4399-83ab-ef61ccf71133"
      },
      "outputs": [],
      "source": [
        "# complining all the steps above into a single function\n",
        "def predict(train, test, predictors, model):\n",
        "    model.fit(train[predictors], train[\"Target\"])\n",
        "    preds = model.predict(test[predictors])\n",
        "    preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
        "    combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
        "    return combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca97d93a-6841-49ef-8f91-25a713baef16",
      "metadata": {
        "id": "ca97d93a-6841-49ef-8f91-25a713baef16"
      },
      "outputs": [],
      "source": [
        "# take 10 years of data since 1 tradin year has 250 days, train the model\n",
        "# 1st model trained on 10 yrs data and then next model training in steps of 1 year\n",
        "# train 10 yrs -> predict 11th year\n",
        "# train 11 yrs -> predict 12th yr\n",
        "# train 12 yrs -> predict 13th yr\n",
        "def backtest(data, model, predictors, start=2500, step=250):\n",
        "    all_predictions = []\n",
        "\n",
        "    for i in range(start, data.shape[0], step):\n",
        "        train = data.iloc[0:i].copy()\n",
        "        test = data.iloc[i:(i+step)].copy()\n",
        "        predictions = predict(train, test, predictors, model)\n",
        "        all_predictions.append(predictions)\n",
        "\n",
        "    return pd.concat(all_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a75261f-d2de-4bc6-9364-54d520c63985",
      "metadata": {
        "id": "8a75261f-d2de-4bc6-9364-54d520c63985"
      },
      "outputs": [],
      "source": [
        "predictions = backtest(sp500, model, predictors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i2QvTX9kYHlF",
      "metadata": {
        "id": "i2QvTX9kYHlF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439d8704-c55d-4d1f-a709-acdc0f485e87",
      "metadata": {
        "id": "439d8704-c55d-4d1f-a709-acdc0f485e87"
      },
      "outputs": [],
      "source": [
        "predictions[\"Predictions\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf105e8f-6be5-4494-9658-233761f1c4f4",
      "metadata": {
        "id": "bf105e8f-6be5-4494-9658-233761f1c4f4"
      },
      "outputs": [],
      "source": [
        "# comparing the target to precdictions\n",
        "precision_score(predictions[\"Target\"], predictions[\"Predictions\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U34cY5rWYQ80",
      "metadata": {
        "id": "U34cY5rWYQ80"
      },
      "outputs": [],
      "source": [
        "new_combined = pd.concat([predictions[\"Target\"], predictions[\"Predictions\"]], axis=1)\n",
        "new_combined.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJRV3up1JQAd",
      "metadata": {
        "id": "yJRV3up1JQAd"
      },
      "source": [
        "Right Now the algo is not performing good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I9hxOjZHe-W2",
      "metadata": {
        "id": "I9hxOjZHe-W2"
      },
      "outputs": [],
      "source": [
        "print(new_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff25a20-375e-444a-b5d3-b558753fc817",
      "metadata": {
        "id": "eff25a20-375e-444a-b5d3-b558753fc817"
      },
      "outputs": [],
      "source": [
        "# checking if the market went up or down\n",
        "# predictions[\"Target\"].value_counts() / predictions.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g0JSqoQPLP3X",
      "metadata": {
        "id": "g0JSqoQPLP3X"
      },
      "source": [
        "## Adding more predictors to the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dlFvj3kLeaX",
      "metadata": {
        "id": "4dlFvj3kLeaX"
      },
      "source": [
        "horizons to look at while predicting\n",
        "\n",
        "the code is creating a new column in the DataFrame or Series (sp500) called trend_column. This column contains the sum of values in the \"Target\" column, but each value is the sum of the previous horizon values, effectively creating a rolling sum with a specified window size. The .shift(1) is used to lag the \"Target\" values by one period before calculating the rolling sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed05ece5-f1f4-443a-b179-33c7e709ea4d",
      "metadata": {
        "id": "ed05ece5-f1f4-443a-b179-33c7e709ea4d"
      },
      "outputs": [],
      "source": [
        "horizons = [2,5,60,250,1000]\n",
        "new_predictors = []\n",
        "\n",
        "for horizon in horizons:\n",
        "    rolling_averages = sp500.rolling(horizon).mean()\n",
        "\n",
        "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
        "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
        "\n",
        "    trend_column = f\"Trend_{horizon}\"\n",
        "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
        "\n",
        "    new_predictors+= [ratio_column, trend_column]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c04ab2d-64ff-4f56-a206-605dcce30372",
      "metadata": {
        "id": "6c04ab2d-64ff-4f56-a206-605dcce30372"
      },
      "outputs": [],
      "source": [
        "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrows_Close\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7b2523-85a4-477d-975d-9cf64b1ff557",
      "metadata": {
        "id": "fd7b2523-85a4-477d-975d-9cf64b1ff557"
      },
      "outputs": [],
      "source": [
        "sp500"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p7Q14GGaQyfz",
      "metadata": {
        "id": "p7Q14GGaQyfz"
      },
      "source": [
        "Training the model again. increasing the estimators and reducing the sample split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283be581-dbe1-4f02-8851-ff1a027b4104",
      "metadata": {
        "id": "283be581-dbe1-4f02-8851-ff1a027b4104"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=200, min_samples_split=50, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a843303c-a247-4f6d-9505-cc711ca95afa",
      "metadata": {
        "id": "a843303c-a247-4f6d-9505-cc711ca95afa"
      },
      "outputs": [],
      "source": [
        "def predict(train, test, predictors, model):\n",
        "    model.fit(train[predictors], train[\"Target\"])\n",
        "    preds = model.predict_proba(test[predictors])[:,1] #return the probability of stock price going up or down\n",
        "    # by deafult the polarising point is 0.5 but to improve the accuracy and make the model more strict, the\n",
        "    # turning point is moved to 0.6 instead\n",
        "    preds[preds >=.6] = 1\n",
        "    preds[preds <.6] = 0\n",
        "    preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
        "    combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
        "    return combined"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p81NzweVSAUi",
      "metadata": {
        "id": "p81NzweVSAUi"
      },
      "source": [
        "instead of using the open close etc, using the new predictors to get better idea of the price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb820946-1275-4914-b6a8-355e96f315b6",
      "metadata": {
        "id": "fb820946-1275-4914-b6a8-355e96f315b6"
      },
      "outputs": [],
      "source": [
        "predictions = backtest(sp500, model, new_predictors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dLviXPKSOgD",
      "metadata": {
        "id": "5dLviXPKSOgD"
      },
      "source": [
        "1.0 dec beacuse of the change in polarity point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73e1816-283a-47ac-af43-4550b80307ef",
      "metadata": {
        "id": "a73e1816-283a-47ac-af43-4550b80307ef"
      },
      "outputs": [],
      "source": [
        "predictions[\"Predictions\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14acc336-4991-4189-bb16-4a8bf53056e1",
      "metadata": {
        "id": "14acc336-4991-4189-bb16-4a8bf53056e1"
      },
      "outputs": [],
      "source": [
        "precision_score(predictions[\"Target\"], predictions[\"Predictions\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4UtrMungSeDA",
      "metadata": {
        "id": "4UtrMungSeDA"
      },
      "source": [
        "WIth jsut looking at the time series data and only the stock prices, the result is good for a base work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b3d365-2157-4229-a785-ae687da0f21f",
      "metadata": {
        "id": "21b3d365-2157-4229-a785-ae687da0f21f"
      },
      "outputs": [],
      "source": [
        "predictions[\"Target\"].value_counts() / predictions.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef08fff5-0dd5-4d86-9d0d-8ce9f7443865",
      "metadata": {
        "id": "ef08fff5-0dd5-4d86-9d0d-8ce9f7443865"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EspxjA26TupL",
      "metadata": {
        "id": "EspxjA26TupL"
      },
      "source": [
        "# Possible Next Steps\n",
        "\n",
        "1. exchanges that are open overnight, so they are trading a few hours earlier than the sp500 so using that data to improve the predicitons\n",
        "1. maybe integrating news\n",
        "1. adding special sectors and predicting on that basis, like how is tech performing, how is oil performing etc\n",
        "1. maybe incrase the resultion to the data, hour basis maybe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d35fd3-7038-4e69-bcbd-bde42c1f5e33",
      "metadata": {
        "id": "b2d35fd3-7038-4e69-bcbd-bde42c1f5e33"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}